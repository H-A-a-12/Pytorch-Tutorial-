{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent Using Autograd - PyTorch Beginner 05.ipynb","provenance":[],"authorship_tag":"ABX9TyOUuHrhS3ax6OvWGTtmHi+0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKT8MQdOYYb3","executionInfo":{"status":"ok","timestamp":1620990593564,"user_tz":-330,"elapsed":1090,"user":{"displayName":"rockstar boy","photoUrl":"","userId":"03282609593521705753"}},"outputId":"da42529a-f40d-443b-b72f-52aa164f2738"},"source":["import numpy as np\n","import torch\n","\n","#x=np.array([1,2,3,4],dtype=np.float32)\n","#y=np.array([5,6,8,7],dtype=np.float32)\n","x=torch.tensor([1,2,3,4],dtype=torch.float32)\n","y=torch.tensor([2, 4, 6, 8],dtype=torch.float32)\n","#w=0.0\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","def forward(x):\n","  return w*x\n","\n","def loss(y_pred,y):\n","  return ((y_pred-y)**2).mean()\n","\n","\n","def gradient(x,y_pred,y):\n","  return np.dot(2*x,(y_pred-y)).mean()\n","\n","\n","\n","print(f\"Prediction Before Training {forward(5)}\")\n","\n","\n","learning_rate=0.01\n","\n","epoch=10\n","for i in range(epoch):\n","  y_pred=forward(x)\n","  l=loss(y_pred,y)\n","\n","  l.backward()\n","\n","  with torch.no_grad():\n","    w -= learning_rate+w.grad\n","\n","  w.grad.zero_()\n","\n","  if epoch % 10 == 0:\n","        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n","\n","print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Prediction Before Training 0.0\n","epoch 11: w = 29.990, loss = 30.00000000\n","epoch 11: w = -389.870, loss = 5875.80078125\n","epoch 11: w = 5488.170, loss = 1151715.75000000\n","epoch 11: w = -76804.383, loss = 225735456.00000000\n","epoch 11: w = 1075291.375, loss = 44244156416.00000000\n","epoch 11: w = -15054049.000, loss = 8671853543424.00000000\n","epoch 11: w = 210756720.000, loss = 1699683378397184.00000000\n","epoch 11: w = -2950594048.000, loss = 333137970083135488.00000000\n","epoch 11: w = 41308315648.000, loss = 65295034577152114688.00000000\n","epoch 11: w = -578316402688.000, loss = 12797827269703023722496.00000000\n","Prediction after training: f(5) = -2891582013440.000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afe8DOH5c6wK","executionInfo":{"status":"ok","timestamp":1620990508467,"user_tz":-330,"elapsed":1782,"user":{"displayName":"rockstar boy","photoUrl":"","userId":"03282609593521705753"}},"outputId":"6db0d63d-f5d3-443d-919b-738ab096cd05"},"source":["import torch\n","\n","# Here we replace the manually computed gradient with autograd\n","\n","# Linear regression\n","# f = w * x \n","\n","# here : f = 2 * x\n","X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n","Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","# model output\n","def forward(x):\n","    return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","    return ((y_pred - y)**2).mean()\n","\n","print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n","\n","# Training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","for epoch in range(n_iters):\n","    # predict = forward pass\n","    y_pred = forward(X)\n","\n","    # loss\n","    l = loss(Y, y_pred)\n","\n","    # calculate gradients = backward pass\n","    l.backward()\n","\n","    # update weights\n","    #w.data = w.data - learning_rate * w.grad\n","    with torch.no_grad():\n","        w -= learning_rate * w.grad\n","    \n","    # zero the gradients after updating\n","    w.grad.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n","\n","print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 0.300, loss = 30.00000000\n","epoch 11: w = 1.665, loss = 1.16278565\n","epoch 21: w = 1.934, loss = 0.04506890\n","epoch 31: w = 1.987, loss = 0.00174685\n","epoch 41: w = 1.997, loss = 0.00006770\n","epoch 51: w = 1.999, loss = 0.00000262\n","epoch 61: w = 2.000, loss = 0.00000010\n","epoch 71: w = 2.000, loss = 0.00000000\n","epoch 81: w = 2.000, loss = 0.00000000\n","epoch 91: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nJlypTOOkDVN"},"source":[""],"execution_count":null,"outputs":[]}]}