{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Convolutional Neural Network (CNN) - PyTorch .ipynb","provenance":[],"authorship_tag":"ABX9TyPQ/I10HdoklOX7ocPVZKsq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"GNARirjE6nyV","executionInfo":{"status":"ok","timestamp":1621234445227,"user_tz":-330,"elapsed":1992,"user":{"displayName":"rockstar boy","photoUrl":"","userId":"03282609593521705753"}}},"source":["import torch \n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpX_tf7ZGl_O","executionInfo":{"status":"ok","timestamp":1621237244538,"user_tz":-330,"elapsed":584570,"user":{"displayName":"rockstar boy","photoUrl":"","userId":"03282609593521705753"}},"outputId":"3d4c0ca4-3214-4312-879b-c314e38f72b4"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","num_epochs=15\n","batch_size=4\n","learning_rate=0.001\n","\n","\n","transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n","\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n","                                          shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n","                                         shuffle=False)\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\n","\n","class ConvNet(nn.Module):\n","  def __init__(self):\n","    super(ConvNet,self).__init__()\n","    self.conv1=nn.Conv2d(3,6,5)# Intput Channel , Output Channel , Number of kernel filter \n","    self.pool=nn.MaxPool2d(2,2) # Size , Stride \n","    self.conv2=nn.Conv2d(6,16,5)\n","    self.fc1=nn.Linear(16*5*5,120)\n","    self.fc2=nn.Linear(120,84)\n","    self.fc3=nn.Linear(84,10)\n","\n","  def forward(self,x):\n","    x=self.pool(F.relu(self.conv1(x)))\n","    x=self.pool(F.relu(self.conv2(x)))\n","    x=x.view(-1,16*5*5)\n","    x=F.relu(self.fc1(x))\n","    x=F.relu(self.fc2(x))\n","    x=self.fc3(x)\n","    return x\n","\n","model =ConvNet().to(device)\n","criterion=nn.CrossEntropyLoss()\n","\n","optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n","\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n","        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 2000 == 0:\n","            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","print('Finished Training')\n","PATH = './cnn.pth'\n","torch.save(model.state_dict(), PATH)\n","\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    n_class_correct = [0 for i in range(10)]\n","    n_class_samples = [0 for i in range(10)]\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        # max returns (value ,index)\n","        _, predicted = torch.max(outputs, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predicted == labels).sum().item()\n","        \n","        for i in range(batch_size):\n","            label = labels[i]\n","            pred = predicted[i]\n","            if (label == pred):\n","                n_class_correct[label] += 1\n","            n_class_samples[label] += 1\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the network: {acc} %')\n","\n","    for i in range(10):\n","        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n","        print(f'Accuracy of {classes[i]}: {acc} %')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch [1/15], Step [2000/12500], Loss: 2.2902\n","Epoch [1/15], Step [4000/12500], Loss: 2.3025\n","Epoch [1/15], Step [6000/12500], Loss: 2.3144\n","Epoch [1/15], Step [8000/12500], Loss: 2.3040\n","Epoch [1/15], Step [10000/12500], Loss: 2.2630\n","Epoch [1/15], Step [12000/12500], Loss: 2.1471\n","Epoch [2/15], Step [2000/12500], Loss: 1.7482\n","Epoch [2/15], Step [4000/12500], Loss: 1.8844\n","Epoch [2/15], Step [6000/12500], Loss: 1.9442\n","Epoch [2/15], Step [8000/12500], Loss: 1.7788\n","Epoch [2/15], Step [10000/12500], Loss: 1.5119\n","Epoch [2/15], Step [12000/12500], Loss: 1.3187\n","Epoch [3/15], Step [2000/12500], Loss: 1.4501\n","Epoch [3/15], Step [4000/12500], Loss: 1.2560\n","Epoch [3/15], Step [6000/12500], Loss: 1.4363\n","Epoch [3/15], Step [8000/12500], Loss: 1.0173\n","Epoch [3/15], Step [10000/12500], Loss: 1.4390\n","Epoch [3/15], Step [12000/12500], Loss: 1.8230\n","Epoch [4/15], Step [2000/12500], Loss: 1.4550\n","Epoch [4/15], Step [4000/12500], Loss: 1.0813\n","Epoch [4/15], Step [6000/12500], Loss: 1.0494\n","Epoch [4/15], Step [8000/12500], Loss: 1.3065\n","Epoch [4/15], Step [10000/12500], Loss: 1.8131\n","Epoch [4/15], Step [12000/12500], Loss: 1.0459\n","Epoch [5/15], Step [2000/12500], Loss: 1.8594\n","Epoch [5/15], Step [4000/12500], Loss: 1.6609\n","Epoch [5/15], Step [6000/12500], Loss: 1.5889\n","Epoch [5/15], Step [8000/12500], Loss: 2.2752\n","Epoch [5/15], Step [10000/12500], Loss: 2.2106\n","Epoch [5/15], Step [12000/12500], Loss: 0.9599\n","Epoch [6/15], Step [2000/12500], Loss: 1.5100\n","Epoch [6/15], Step [4000/12500], Loss: 1.9655\n","Epoch [6/15], Step [6000/12500], Loss: 1.0456\n","Epoch [6/15], Step [8000/12500], Loss: 1.2412\n","Epoch [6/15], Step [10000/12500], Loss: 1.7443\n","Epoch [6/15], Step [12000/12500], Loss: 0.6039\n","Epoch [7/15], Step [2000/12500], Loss: 1.8717\n","Epoch [7/15], Step [4000/12500], Loss: 1.1737\n","Epoch [7/15], Step [6000/12500], Loss: 0.8734\n","Epoch [7/15], Step [8000/12500], Loss: 1.3561\n","Epoch [7/15], Step [10000/12500], Loss: 0.9392\n","Epoch [7/15], Step [12000/12500], Loss: 0.6737\n","Epoch [8/15], Step [2000/12500], Loss: 0.4748\n","Epoch [8/15], Step [4000/12500], Loss: 3.4562\n","Epoch [8/15], Step [6000/12500], Loss: 0.9673\n","Epoch [8/15], Step [8000/12500], Loss: 1.5052\n","Epoch [8/15], Step [10000/12500], Loss: 1.3359\n","Epoch [8/15], Step [12000/12500], Loss: 1.0989\n","Epoch [9/15], Step [2000/12500], Loss: 1.2946\n","Epoch [9/15], Step [4000/12500], Loss: 0.4478\n","Epoch [9/15], Step [6000/12500], Loss: 0.5446\n","Epoch [9/15], Step [8000/12500], Loss: 1.0682\n","Epoch [9/15], Step [10000/12500], Loss: 1.1595\n","Epoch [9/15], Step [12000/12500], Loss: 1.2904\n","Epoch [10/15], Step [2000/12500], Loss: 1.3460\n","Epoch [10/15], Step [4000/12500], Loss: 1.2251\n","Epoch [10/15], Step [6000/12500], Loss: 1.3646\n","Epoch [10/15], Step [8000/12500], Loss: 1.2690\n","Epoch [10/15], Step [10000/12500], Loss: 1.5227\n","Epoch [10/15], Step [12000/12500], Loss: 0.6494\n","Epoch [11/15], Step [2000/12500], Loss: 0.7366\n","Epoch [11/15], Step [4000/12500], Loss: 0.9411\n","Epoch [11/15], Step [6000/12500], Loss: 0.6861\n","Epoch [11/15], Step [8000/12500], Loss: 0.5058\n","Epoch [11/15], Step [10000/12500], Loss: 0.9981\n","Epoch [11/15], Step [12000/12500], Loss: 0.5597\n","Epoch [12/15], Step [2000/12500], Loss: 1.3877\n","Epoch [12/15], Step [4000/12500], Loss: 0.9084\n","Epoch [12/15], Step [6000/12500], Loss: 1.6165\n","Epoch [12/15], Step [8000/12500], Loss: 0.4770\n","Epoch [12/15], Step [10000/12500], Loss: 1.2219\n","Epoch [12/15], Step [12000/12500], Loss: 1.2949\n","Epoch [13/15], Step [2000/12500], Loss: 1.0842\n","Epoch [13/15], Step [4000/12500], Loss: 0.9861\n","Epoch [13/15], Step [6000/12500], Loss: 0.8401\n","Epoch [13/15], Step [8000/12500], Loss: 1.4144\n","Epoch [13/15], Step [10000/12500], Loss: 1.6247\n","Epoch [13/15], Step [12000/12500], Loss: 0.9057\n","Epoch [14/15], Step [2000/12500], Loss: 1.3968\n","Epoch [14/15], Step [4000/12500], Loss: 1.4375\n","Epoch [14/15], Step [6000/12500], Loss: 1.0459\n","Epoch [14/15], Step [8000/12500], Loss: 1.2397\n","Epoch [14/15], Step [10000/12500], Loss: 0.9074\n","Epoch [14/15], Step [12000/12500], Loss: 0.2501\n","Epoch [15/15], Step [2000/12500], Loss: 0.3644\n","Epoch [15/15], Step [4000/12500], Loss: 0.3962\n","Epoch [15/15], Step [6000/12500], Loss: 1.6391\n","Epoch [15/15], Step [8000/12500], Loss: 2.0950\n","Epoch [15/15], Step [10000/12500], Loss: 1.4106\n","Epoch [15/15], Step [12000/12500], Loss: 0.9828\n","Finished Training\n","Accuracy of the network: 61.96 %\n","Accuracy of plane: 70.8 %\n","Accuracy of car: 63.6 %\n","Accuracy of bird: 50.3 %\n","Accuracy of cat: 47.8 %\n","Accuracy of deer: 58.1 %\n","Accuracy of dog: 53.2 %\n","Accuracy of frog: 63.0 %\n","Accuracy of horse: 63.1 %\n","Accuracy of ship: 76.4 %\n","Accuracy of truck: 73.3 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WqQ0rZ47NnIr"},"source":[""],"execution_count":null,"outputs":[]}]}