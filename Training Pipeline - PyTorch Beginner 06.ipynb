{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training Pipeline - PyTorch Beginner 06.ipynb","provenance":[],"authorship_tag":"ABX9TyMl8F9WOnDqbvwcDQe4hTx+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CqwRMTzClM-b","executionInfo":{"status":"ok","timestamp":1620991343889,"user_tz":-330,"elapsed":1361,"user":{"displayName":"rockstar boy","photoUrl":"","userId":"03282609593521705753"}},"outputId":"1ce00c5a-e667-4bcc-83df-e812ad276d62"},"source":["import torch \n","import torch.nn as nn\n","\n","x=torch.tensor([1,2,3,4],dtype=torch.float32)\n","y=torch.tensor([5,7,8,9],dtype=torch.float32)\n","\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","def forward(x):\n","  return w*x\n","\n","print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n","\n","learning_rate = 0.01\n","n_iters = 100\n","\n","\n","loss=nn.MSELoss()\n","optimizer = torch.optim.SGD([w], lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  y_prediction=forward(x)\n","  l=loss(y_prediction,y)\n","\n","  optimizer.step()\n","\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","      print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n","\n","print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Prediction before training: f(5) = 0.000\n","epoch  1 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  11 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  21 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  31 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  41 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  51 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  61 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  71 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  81 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","epoch  91 : w =  tensor(0., requires_grad=True)  loss =  tensor(54.7500, grad_fn=<MseLossBackward>)\n","Prediction after training: f(5) = 0.000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5O-LquSnDF8","executionInfo":{"status":"ok","timestamp":1620991435840,"user_tz":-330,"elapsed":1528,"user":{"displayName":"rockstar boy","photoUrl":"","userId":"03282609593521705753"}},"outputId":"0d1f2376-42a3-4c37-aa1f-c80b357ea6bd"},"source":["import torch\n","import torch.nn as nn\n","\n","# Linear regression\n","# f = w * x \n","\n","# here : f = 2 * x\n","\n","# 0) Training samples, watch the shape!\n","X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","\n","n_samples, n_features = X.shape\n","print(f'#samples: {n_samples}, #features: {n_features}')\n","# 0) create a test sample\n","X_test = torch.tensor([5], dtype=torch.float32)\n","\n","# 1) Design Model, the model has to implement the forward pass!\n","# Here we can use a built-in model from PyTorch\n","input_size = n_features\n","output_size = n_features\n","\n","# we can call this model with samples X\n","model = nn.Linear(input_size, output_size)\n","\n","'''\n","class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LinearRegression, self).__init__()\n","        # define diferent layers\n","        self.lin = nn.Linear(input_dim, output_dim)\n","    def forward(self, x):\n","        return self.lin(x)\n","model = LinearRegression(input_size, output_size)\n","'''\n","\n","print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n","\n","# 2) Define loss and optimizer\n","learning_rate = 0.01\n","n_iters = 100\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# 3) Training loop\n","for epoch in range(n_iters):\n","    # predict = forward pass with our model\n","    y_predicted = model(X)\n","\n","    # loss\n","    l = loss(Y, y_predicted)\n","\n","    # calculate gradients = backward pass\n","    l.backward()\n","\n","    # update weights\n","    optimizer.step()\n","\n","    # zero the gradients after updating\n","    optimizer.zero_grad()\n","\n","    if epoch % 10 == 0:\n","        [w, b] = model.parameters() # unpack parameters\n","        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n","\n","print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["#samples: 4, #features: 1\n","Prediction before training: f(5) = -5.681\n","epoch  1 : w =  -0.4653448164463043  loss =  tensor(79.7261, grad_fn=<MseLossBackward>)\n","epoch  11 : w =  1.5803296566009521  loss =  tensor(2.0638, grad_fn=<MseLossBackward>)\n","epoch  21 : w =  1.9100574254989624  loss =  tensor(0.0544, grad_fn=<MseLossBackward>)\n","epoch  31 : w =  1.9637565612792969  loss =  tensor(0.0024, grad_fn=<MseLossBackward>)\n","epoch  41 : w =  1.9730372428894043  loss =  tensor(0.0010, grad_fn=<MseLossBackward>)\n","epoch  51 : w =  1.9751542806625366  loss =  tensor(0.0009, grad_fn=<MseLossBackward>)\n","epoch  61 : w =  1.9761006832122803  loss =  tensor(0.0008, grad_fn=<MseLossBackward>)\n","epoch  71 : w =  1.976840853691101  loss =  tensor(0.0008, grad_fn=<MseLossBackward>)\n","epoch  81 : w =  1.9775304794311523  loss =  tensor(0.0007, grad_fn=<MseLossBackward>)\n","epoch  91 : w =  1.9781949520111084  loss =  tensor(0.0007, grad_fn=<MseLossBackward>)\n","Prediction after training: f(5) = 9.956\n"],"name":"stdout"}]}]}